{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9980b9f0",
   "metadata": {},
   "source": [
    "tsv(Tab-separated values): 데이터를 탭 기준으로 구분한 파일 형식   \n",
    "\n",
    "자연어 데이터에는 콤마가 들어있는 경우가 많아 => csv를 데이터 구분의 기준으로 사용하면 코퍼스의 형태가 망가져\n",
    "  \n",
    "자연어 데이터를 저장할 때에는 tsv 형식을 많이 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1169cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b69ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\semin\\AppData\\Local\\Temp\\ipykernel_33032\\2293128741.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('../data/imdb.tsv', delimiter='\\\\t')\n"
     ]
    }
   ],
   "source": [
    "#delimiter 속성을 '\\t'로 지정해야 합니다.\n",
    "df = pd.read_csv('../data/imdb.tsv', delimiter='\\\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d13530e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Minor Spoilers In New York, Joan Barnard (Elvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Jennifer Ehle was sparkling in \\\"\"Pride and P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amy Poehler is a terrific comedian on Saturday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"A plane carrying employees of a large biotech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A well made, gritty science fiction movie, it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Incredibly dumb and utterly predictable story...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  \"Watching Time Chasers, it obvious that it was...\n",
       "1  I saw this film about 20 years ago and remembe...\n",
       "2  Minor Spoilers In New York, Joan Barnard (Elvi...\n",
       "3  I went to see this film with a great deal of e...\n",
       "4  \"Yes, I agree with everyone on this site this ...\n",
       "5  \"Jennifer Ehle was sparkling in \\\"\"Pride and P...\n",
       "6  Amy Poehler is a terrific comedian on Saturday...\n",
       "7  \"A plane carrying employees of a large biotech...\n",
       "8  A well made, gritty science fiction movie, it ...\n",
       "9  \"Incredibly dumb and utterly predictable story..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96bafb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#소문자로 정규화\n",
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e59d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7235aa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\semin\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['word_tokens'] = df['review'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57d7c5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['word_tokens'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a89915c",
   "metadata": {},
   "source": [
    "# 데이터 정제\n",
    "각 코퍼스별로 등장 빈도가 1회 이하, 단어의 길이가 2 이하, 그리고 NLTK에서 기본 제공하는 불용어에 해당하는 단어들을 정제해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cb545a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64b46a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\semin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\semin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\semin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from preprocess import clean_by_freq\n",
    "from preprocess import clean_by_len\n",
    "from preprocess import clean_by_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d22e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0083ddf2",
   "metadata": {},
   "source": [
    "# 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0df46238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"watching time chasers, it obvious that it was made by a bunch of friends.',\n",
       " 'maybe they were sitting around one day in film school and said, \\\\\"\"hey, let\\'s pool our money together and make a really bad movie!\\\\\"\" or something like that.',\n",
       " 'what ever they said, they still ended up making a really bad movie--dull story, bad script, lame acting, poor cinematography, bottom of the barrel stock music, etc.',\n",
       " \"all corners were cut, except the one that would have prevented this film's release.\",\n",
       " 'life\\'s like that.\"']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocess import sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "df['sent_tokens'] = df['review'].apply(sent_tokenize)\n",
    "df['sent_tokens'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0420b",
   "metadata": {},
   "source": [
    "# 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39b18de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import pos_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73aebe2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('``', '``'),\n",
       " ('watching', 'JJ'),\n",
       " ('time', 'NN'),\n",
       " ('chasers', 'NNS'),\n",
       " (',', ','),\n",
       " ('it', 'PRP'),\n",
       " ('obvious', 'VBZ'),\n",
       " ('that', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('made', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('bunch', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('friends', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('maybe', 'RB'),\n",
       " ('they', 'PRP'),\n",
       " ('were', 'VBD'),\n",
       " ('sitting', 'VBG'),\n",
       " ('around', 'IN'),\n",
       " ('one', 'CD'),\n",
       " ('day', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('film', 'NN'),\n",
       " ('school', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('said', 'VBD'),\n",
       " (',', ','),\n",
       " ('\\\\', 'FW'),\n",
       " (\"''\", \"''\"),\n",
       " (\"''\", \"''\"),\n",
       " ('hey', 'NN'),\n",
       " (',', ','),\n",
       " ('let', 'VB'),\n",
       " (\"'s\", 'POS'),\n",
       " ('pool', 'VB'),\n",
       " ('our', 'PRP$'),\n",
       " ('money', 'NN'),\n",
       " ('together', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('make', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('really', 'RB'),\n",
       " ('bad', 'JJ'),\n",
       " ('movie', 'NN'),\n",
       " ('!', '.'),\n",
       " ('\\\\', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " (\"''\", \"''\"),\n",
       " ('or', 'CC'),\n",
       " ('something', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('.', '.'),\n",
       " ('what', 'WP'),\n",
       " ('ever', 'RB'),\n",
       " ('they', 'PRP'),\n",
       " ('said', 'VBD'),\n",
       " (',', ','),\n",
       " ('they', 'PRP'),\n",
       " ('still', 'RB'),\n",
       " ('ended', 'VBD'),\n",
       " ('up', 'RP'),\n",
       " ('making', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('really', 'RB'),\n",
       " ('bad', 'JJ'),\n",
       " ('movie', 'NN'),\n",
       " ('--', ':'),\n",
       " ('dull', 'JJ'),\n",
       " ('story', 'NN'),\n",
       " (',', ','),\n",
       " ('bad', 'JJ'),\n",
       " ('script', 'NN'),\n",
       " (',', ','),\n",
       " ('lame', 'NN'),\n",
       " ('acting', 'NN'),\n",
       " (',', ','),\n",
       " ('poor', 'JJ'),\n",
       " ('cinematography', 'NN'),\n",
       " (',', ','),\n",
       " ('bottom', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('barrel', 'NN'),\n",
       " ('stock', 'NN'),\n",
       " ('music', 'NN'),\n",
       " (',', ','),\n",
       " ('etc', 'FW'),\n",
       " ('.', '.'),\n",
       " ('all', 'DT'),\n",
       " ('corners', 'NNS'),\n",
       " ('were', 'VBD'),\n",
       " ('cut', 'VBN'),\n",
       " (',', ','),\n",
       " ('except', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('one', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('would', 'MD'),\n",
       " ('have', 'VB'),\n",
       " ('prevented', 'VBN'),\n",
       " ('this', 'DT'),\n",
       " ('film', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('release', 'NN'),\n",
       " ('.', '.'),\n",
       " ('life', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('like', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('.', '.'),\n",
       " (\"''\", \"''\")]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pos_tagged_tokens']= df['sent_tokens'].apply(pos_tagger)\n",
    "df['pos_tagged_tokens'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4121c41",
   "metadata": {},
   "source": [
    "# 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34e2c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import words_lemmatizer\n",
    "\n",
    "df['lemmatized_tokens'] = df['pos_tagged_tokens'].apply(words_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42bcc239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['``',\n",
       " 'watching',\n",
       " 'time',\n",
       " 'chaser',\n",
       " ',',\n",
       " 'it',\n",
       " 'obvious',\n",
       " 'that',\n",
       " 'it',\n",
       " 'be',\n",
       " 'make',\n",
       " 'by',\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'friend',\n",
       " '.',\n",
       " 'maybe',\n",
       " 'they',\n",
       " 'be',\n",
       " 'sit',\n",
       " 'around',\n",
       " 'one',\n",
       " 'day',\n",
       " 'in',\n",
       " 'film',\n",
       " 'school',\n",
       " 'and',\n",
       " 'say',\n",
       " ',',\n",
       " '\\\\',\n",
       " \"''\",\n",
       " \"''\",\n",
       " 'hey',\n",
       " ',',\n",
       " 'let',\n",
       " \"'s\",\n",
       " 'pool',\n",
       " 'our',\n",
       " 'money',\n",
       " 'together',\n",
       " 'and',\n",
       " 'make',\n",
       " 'a',\n",
       " 'really',\n",
       " 'bad',\n",
       " 'movie',\n",
       " '!',\n",
       " '\\\\',\n",
       " \"''\",\n",
       " \"''\",\n",
       " 'or',\n",
       " 'something',\n",
       " 'like',\n",
       " 'that',\n",
       " '.',\n",
       " 'what',\n",
       " 'ever',\n",
       " 'they',\n",
       " 'say',\n",
       " ',',\n",
       " 'they',\n",
       " 'still',\n",
       " 'end',\n",
       " 'up',\n",
       " 'make',\n",
       " 'a',\n",
       " 'really',\n",
       " 'bad',\n",
       " 'movie',\n",
       " '--',\n",
       " 'dull',\n",
       " 'story',\n",
       " ',',\n",
       " 'bad',\n",
       " 'script',\n",
       " ',',\n",
       " 'lame',\n",
       " 'acting',\n",
       " ',',\n",
       " 'poor',\n",
       " 'cinematography',\n",
       " ',',\n",
       " 'bottom',\n",
       " 'of',\n",
       " 'the',\n",
       " 'barrel',\n",
       " 'stock',\n",
       " 'music',\n",
       " ',',\n",
       " 'etc',\n",
       " '.',\n",
       " 'all',\n",
       " 'corner',\n",
       " 'be',\n",
       " 'cut',\n",
       " ',',\n",
       " 'except',\n",
       " 'the',\n",
       " 'one',\n",
       " 'that',\n",
       " 'would',\n",
       " 'have',\n",
       " 'prevent',\n",
       " 'this',\n",
       " 'film',\n",
       " \"'s\",\n",
       " 'release',\n",
       " '.',\n",
       " 'life',\n",
       " \"'s\",\n",
       " 'like',\n",
       " 'that',\n",
       " '.',\n",
       " \"''\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemmatized_tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72754451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe058414",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tokens'] = df['word_tokens'].apply(lambda x: clean_by_freq(x, 1))\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1280919d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[one, film, said, really, bad, movie, like, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[film, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[went, film, film, went, jump, send, n't, jump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[site, movie, bad, even, movie, made, movie, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ehle, northam, wonderful, wonderful, ehle, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[role, movie, n't, author, book, author, autho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[plane, ceo, search, rescue, mission, ceo, har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[gritty, movie, sci-fi, good, suspense, movie,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[girl, girl]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      cleaned_tokens\n",
       "0  [one, film, said, really, bad, movie, like, sa...\n",
       "1                                       [film, film]\n",
       "2  [new, york, joan, barnard, elvire, audrey, bar...\n",
       "3  [went, film, film, went, jump, send, n't, jump...\n",
       "4  [site, movie, bad, even, movie, made, movie, s...\n",
       "5  [ehle, northam, wonderful, wonderful, ehle, no...\n",
       "6  [role, movie, n't, author, book, author, autho...\n",
       "7  [plane, ceo, search, rescue, mission, ceo, har...\n",
       "8  [gritty, movie, sci-fi, good, suspense, movie,...\n",
       "9                                       [girl, girl]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['cleaned_tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5521fe21",
   "metadata": {},
   "source": [
    "# 자연어 전처리 후 통합하기\n",
    "*  리스트의 요소들을 하나로 합칠 때에는 join() 함수를 '구분자'.join(리스트) 형태로 사용\n",
    "* 단어 토큰들을 띄어쓰기로 구분하기 위해 구분자를 공백(' ')으로 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "758048a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(sentence):\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a415acc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combined_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one film said really bad movie like said reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>film film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new york joan barnard elvire audrey barnard jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>went film film went jump send n't jump radio n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>site movie bad even movie made movie special m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ehle northam wonderful wonderful ehle northam ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>role movie n't author book author author role ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>plane ceo search rescue mission ceo harlan kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gritty movie sci-fi good suspense movie sci-fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>girl girl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     combined_corpus\n",
       "0  one film said really bad movie like said reall...\n",
       "1                                          film film\n",
       "2  new york joan barnard elvire audrey barnard jo...\n",
       "3  went film film went jump send n't jump radio n...\n",
       "4  site movie bad even movie made movie special m...\n",
       "5  ehle northam wonderful wonderful ehle northam ...\n",
       "6  role movie n't author book author author role ...\n",
       "7  plane ceo search rescue mission ceo harlan kno...\n",
       "8  gritty movie sci-fi good suspense movie sci-fi...\n",
       "9                                          girl girl"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['combined_corpus']=df['cleaned_tokens'].apply(combine)\n",
    "df[['combined_corpus']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6edf253",
   "metadata": {},
   "source": [
    "# 정수 인코딩\n",
    "* 정수 인코딩: 토큰화된 각 단어에 특정 정수를 맵핑하여 고유 번호로 사용하는 방법  \n",
    "- 단어의 등장 빈도를 기준으로 정렬한 다음 인덱스를 부여하는 방식\n",
    "- 모든 전처리 과정이 끝난 코퍼스를 가지고 정수 인코딩을 해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f18a6550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[one, film, said, really, bad, movie, like, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[film, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[went, film, film, went, jump, send, n't, jump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[site, movie, bad, even, movie, made, movie, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ehle, northam, wonderful, wonderful, ehle, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[role, movie, n't, author, book, author, autho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[plane, ceo, search, rescue, mission, ceo, har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[gritty, movie, sci-fi, good, suspense, movie,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[girl, girl]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      cleaned_tokens\n",
       "0  [one, film, said, really, bad, movie, like, sa...\n",
       "1                                       [film, film]\n",
       "2  [new, york, joan, barnard, elvire, audrey, bar...\n",
       "3  [went, film, film, went, jump, send, n't, jump...\n",
       "4  [site, movie, bad, even, movie, made, movie, s...\n",
       "5  [ehle, northam, wonderful, wonderful, ehle, no...\n",
       "6  [role, movie, n't, author, book, author, autho...\n",
       "7  [plane, ceo, search, rescue, mission, ceo, har...\n",
       "8  [gritty, movie, sci-fi, good, suspense, movie,...\n",
       "9                                       [girl, girl]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['cleaned_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "962646ca",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 9),\n",
       " ('jim', 7),\n",
       " ('stand-up', 3),\n",
       " ('days', 3),\n",
       " ('really', 3),\n",
       " ('terrible', 3),\n",
       " ('site', 2),\n",
       " ('bad', 2),\n",
       " ('even', 2),\n",
       " ('made', 2),\n",
       " ('special', 2),\n",
       " ('like', 2),\n",
       " ('actor', 2),\n",
       " ('love', 2),\n",
       " ('stand', 2)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Example test\n",
    "# tokens = df['cleaned_tokens'][4]\n",
    "# vocab = Counter(tokens)\n",
    "# vocab = vocab.most_common()\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dd47725",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie': 1,\n",
       " 'jim': 2,\n",
       " 'stand-up': 3,\n",
       " 'days': 4,\n",
       " 'really': 5,\n",
       " 'terrible': 6,\n",
       " 'site': 7,\n",
       " 'bad': 8,\n",
       " 'even': 9,\n",
       " 'made': 10,\n",
       " 'special': 11,\n",
       " 'like': 12,\n",
       " 'actor': 13,\n",
       " 'love': 14,\n",
       " 'stand': 15}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_to_idx ={}\n",
    "# i = 0\n",
    "# for (word, frequency) in vocab :\n",
    "#     i += 1\n",
    "#     word_to_idx[word] =  i\n",
    "\n",
    "# word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8b32d7f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 11,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 10,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 12,\n",
       " 2,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 4,\n",
       " 11,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 7,\n",
       " 14,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 13,\n",
       " 1,\n",
       " 15,\n",
       " 2,\n",
       " 12,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 1]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoded_idx = []\n",
    "# for token in tokens:\n",
    "#     idx = word_to_idx[token]\n",
    "#     encoded_idx.append(idx)\n",
    "\n",
    "# encoded_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff7b6e07",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'film',\n",
       " 'said',\n",
       " 'really',\n",
       " 'bad',\n",
       " 'movie',\n",
       " 'like',\n",
       " 'said',\n",
       " 'really',\n",
       " 'bad',\n",
       " 'movie',\n",
       " 'bad',\n",
       " 'one',\n",
       " 'film',\n",
       " 'like',\n",
       " 'film',\n",
       " 'film',\n",
       " 'new',\n",
       " 'york',\n",
       " 'joan',\n",
       " 'barnard',\n",
       " 'elvire',\n",
       " 'audrey',\n",
       " 'barnard',\n",
       " 'john',\n",
       " 'saxon',\n",
       " 'italy',\n",
       " 'etruscan',\n",
       " 'tomb',\n",
       " 'joan',\n",
       " 'italy',\n",
       " 'colleague',\n",
       " 'italy',\n",
       " 'maggots',\n",
       " 'maggots',\n",
       " 'joan',\n",
       " 'tomb',\n",
       " 'colleague',\n",
       " 'story',\n",
       " 'new',\n",
       " 'york',\n",
       " 'joan',\n",
       " 'colleague',\n",
       " 'romantic',\n",
       " 'end',\n",
       " 'time',\n",
       " 'watching',\n",
       " 'story',\n",
       " 'romantic',\n",
       " 'end',\n",
       " 'elvire',\n",
       " 'audrey',\n",
       " 'john',\n",
       " 'saxon',\n",
       " 'maggots',\n",
       " 'watching',\n",
       " 'etrusco',\n",
       " 'time',\n",
       " 'etrusco',\n",
       " 'etruscan',\n",
       " 'went',\n",
       " 'film',\n",
       " 'film',\n",
       " 'went',\n",
       " 'jump',\n",
       " 'send',\n",
       " \"n't\",\n",
       " 'jump',\n",
       " 'radio',\n",
       " \"n't\",\n",
       " 'send',\n",
       " 'reporters',\n",
       " 'fear',\n",
       " 'fear',\n",
       " 'radio',\n",
       " 'reporters',\n",
       " \"n't\",\n",
       " 'radio',\n",
       " \"n't\",\n",
       " \"n't\",\n",
       " 'site',\n",
       " 'movie',\n",
       " 'bad',\n",
       " 'even',\n",
       " 'movie',\n",
       " 'made',\n",
       " 'movie',\n",
       " 'special',\n",
       " 'movie',\n",
       " 'movie',\n",
       " 'movie',\n",
       " 'jim',\n",
       " 'made',\n",
       " 'stand-up',\n",
       " 'days',\n",
       " 'stand-up',\n",
       " 'jim',\n",
       " 'like',\n",
       " 'jim',\n",
       " 'actor',\n",
       " 'love',\n",
       " 'stand',\n",
       " 'days',\n",
       " 'special',\n",
       " 'jim',\n",
       " 'days',\n",
       " 'even',\n",
       " 'site',\n",
       " 'love',\n",
       " 'jim',\n",
       " 'stand-up',\n",
       " 'jim',\n",
       " 'actor',\n",
       " 'movie',\n",
       " 'stand',\n",
       " 'jim',\n",
       " 'like',\n",
       " 'really',\n",
       " 'terrible',\n",
       " 'really',\n",
       " 'terrible',\n",
       " 'movie',\n",
       " 'terrible',\n",
       " 'really',\n",
       " 'bad',\n",
       " 'movie',\n",
       " 'ehle',\n",
       " 'northam',\n",
       " 'wonderful',\n",
       " 'wonderful',\n",
       " 'ehle',\n",
       " 'northam',\n",
       " 'lust',\n",
       " 'lust',\n",
       " 'ehle',\n",
       " 'northam',\n",
       " 'role',\n",
       " 'movie',\n",
       " \"n't\",\n",
       " 'author',\n",
       " 'book',\n",
       " 'author',\n",
       " 'author',\n",
       " 'role',\n",
       " \"n't\",\n",
       " 'queen',\n",
       " 'corn',\n",
       " 'corn',\n",
       " 'queen',\n",
       " 'author',\n",
       " 'book',\n",
       " 'movie',\n",
       " \"n't\",\n",
       " 'plane',\n",
       " 'ceo',\n",
       " 'search',\n",
       " 'rescue',\n",
       " 'mission',\n",
       " 'ceo',\n",
       " 'harlan',\n",
       " 'knowles',\n",
       " 'lance',\n",
       " 'henriksen',\n",
       " 'search',\n",
       " 'rescue',\n",
       " 'mission',\n",
       " 'knowles',\n",
       " 'trying',\n",
       " 'rescue',\n",
       " 'woods',\n",
       " 'film',\n",
       " 'one',\n",
       " 'lance',\n",
       " 'henriksen',\n",
       " 'one',\n",
       " 'two',\n",
       " 'could',\n",
       " 'easily',\n",
       " 'decent',\n",
       " 'film',\n",
       " 'two',\n",
       " \"'re\",\n",
       " 'quastel',\n",
       " 'film',\n",
       " 'calling',\n",
       " 'sasquatch',\n",
       " 'quastel',\n",
       " 'try',\n",
       " 'time',\n",
       " 'try',\n",
       " 'potential',\n",
       " 'material',\n",
       " 'related',\n",
       " 'plane',\n",
       " 'trying',\n",
       " 'material',\n",
       " 'related',\n",
       " 'monster',\n",
       " 'exposition',\n",
       " 'dialogue',\n",
       " 'potential',\n",
       " 'far',\n",
       " 'monster',\n",
       " 'costume',\n",
       " 'get',\n",
       " 'see',\n",
       " 'character',\n",
       " 'woods',\n",
       " 'could',\n",
       " 'quastel',\n",
       " 'would',\n",
       " 'time',\n",
       " 'monster',\n",
       " 'scenes',\n",
       " 'decent',\n",
       " 'dialogue',\n",
       " 'could',\n",
       " 'easily',\n",
       " 'effective',\n",
       " 'sasquatch',\n",
       " 'made',\n",
       " 'reason',\n",
       " 'quastel',\n",
       " 'thinks',\n",
       " 'good',\n",
       " 'idea',\n",
       " 'dialogue',\n",
       " 'scenes',\n",
       " 'time',\n",
       " 'see',\n",
       " 'lines',\n",
       " 'scene',\n",
       " 'lines',\n",
       " 'scene',\n",
       " 'back',\n",
       " 'back',\n",
       " 'reason',\n",
       " 'thinks',\n",
       " 'good',\n",
       " 'idea',\n",
       " 'use',\n",
       " 'use',\n",
       " 'dialogue',\n",
       " 'whether',\n",
       " 'idea',\n",
       " 'time',\n",
       " 'irrelevant',\n",
       " 'comments',\n",
       " 'whether',\n",
       " 'irrelevant',\n",
       " 'comments',\n",
       " 'one',\n",
       " 'time',\n",
       " 'reason',\n",
       " \"n't\",\n",
       " 'whether',\n",
       " 'scenes',\n",
       " 'random',\n",
       " 'scenes',\n",
       " \"'re\",\n",
       " 'shown',\n",
       " 'appear',\n",
       " 'random',\n",
       " 'important',\n",
       " 'either',\n",
       " 'never',\n",
       " 'appear',\n",
       " \"'re\",\n",
       " 'far',\n",
       " 'scenes',\n",
       " 'reason',\n",
       " 'left',\n",
       " 'scene',\n",
       " 'film',\n",
       " 'either',\n",
       " 'needs',\n",
       " 'exposition',\n",
       " 'needs',\n",
       " 'important',\n",
       " 'monster',\n",
       " 'could',\n",
       " 'easily',\n",
       " 'shown',\n",
       " 'reason',\n",
       " 'character',\n",
       " 'left',\n",
       " 'even',\n",
       " 'though',\n",
       " 'reason',\n",
       " 'scene',\n",
       " 'even',\n",
       " 'though',\n",
       " 'never',\n",
       " 'reason',\n",
       " 'calling',\n",
       " 'harlan',\n",
       " 'knowles',\n",
       " 'like',\n",
       " \"'re\",\n",
       " 'reason',\n",
       " 'quastel',\n",
       " 'monster',\n",
       " 'scenes',\n",
       " 'even',\n",
       " 'though',\n",
       " 'costume',\n",
       " \"n't\",\n",
       " 'bad',\n",
       " 'would',\n",
       " 'effective',\n",
       " 'bad',\n",
       " 'could',\n",
       " 'get',\n",
       " 'idea',\n",
       " 'like',\n",
       " 'film',\n",
       " 'better',\n",
       " 'henriksen',\n",
       " 'hiking',\n",
       " 'hiking',\n",
       " 'film',\n",
       " 'one',\n",
       " 'could',\n",
       " \"n't\",\n",
       " 'time',\n",
       " 'better',\n",
       " 'quastel',\n",
       " 'made',\n",
       " 'gritty',\n",
       " 'movie',\n",
       " 'sci-fi',\n",
       " 'good',\n",
       " 'suspense',\n",
       " 'movie',\n",
       " 'sci-fi',\n",
       " \"'re\",\n",
       " 'looking',\n",
       " \"'re\",\n",
       " 'looking',\n",
       " 'good',\n",
       " 'gritty',\n",
       " 'sci-fi',\n",
       " 'good',\n",
       " 'suspense',\n",
       " 'movie',\n",
       " 'good',\n",
       " 'girl',\n",
       " 'girl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모든 로우에 있는 토큰들을 합쳐서 빈도를 계산할 때에는 sum() 함수가 사용됩니다. \n",
    "#sum() 함수에 모든 코퍼스들과 []를 파라미터로 넘겨 주면 하나의 합쳐진 토큰 리스트가 결과로 나옵니다.\n",
    "tokens = sum(df['cleaned_tokens'], [])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1412760",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie': 1, 'film': 2, \"n't\": 3, 'time': 4, 'reason': 5, 'bad': 6, 'jim': 7, 'one': 8, 'like': 9, 'could': 10, \"'re\": 11, 'quastel': 12, 'scenes': 13, 'good': 14, 'really': 15, 'even': 16, 'monster': 17, 'joan': 18, 'made': 19, 'author': 20, 'dialogue': 21, 'idea': 22, 'scene': 23, 'italy': 24, 'colleague': 25, 'maggots': 26, 'radio': 27, 'stand-up': 28, 'days': 29, 'terrible': 30, 'ehle': 31, 'northam': 32, 'rescue': 33, 'knowles': 34, 'henriksen': 35, 'easily': 36, 'whether': 37, 'though': 38, 'sci-fi': 39, 'said': 40, 'new': 41, 'york': 42, 'barnard': 43, 'elvire': 44, 'audrey': 45, 'john': 46, 'saxon': 47, 'etruscan': 48, 'tomb': 49, 'story': 50, 'romantic': 51, 'end': 52, 'watching': 53, 'etrusco': 54, 'went': 55, 'jump': 56, 'send': 57, 'reporters': 58, 'fear': 59, 'site': 60, 'special': 61, 'actor': 62, 'love': 63, 'stand': 64, 'wonderful': 65, 'lust': 66, 'role': 67, 'book': 68, 'queen': 69, 'corn': 70, 'plane': 71, 'ceo': 72, 'search': 73, 'mission': 74, 'harlan': 75, 'lance': 76, 'trying': 77, 'woods': 78, 'two': 79, 'decent': 80, 'calling': 81, 'sasquatch': 82, 'try': 83, 'potential': 84, 'material': 85, 'related': 86, 'exposition': 87, 'far': 88, 'costume': 89, 'get': 90, 'see': 91, 'character': 92, 'would': 93, 'effective': 94, 'thinks': 95, 'lines': 96, 'back': 97, 'use': 98, 'irrelevant': 99, 'comments': 100, 'random': 101, 'shown': 102, 'appear': 103, 'important': 104, 'either': 105, 'never': 106, 'left': 107, 'needs': 108, 'better': 109, 'hiking': 110, 'gritty': 111, 'suspense': 112, 'looking': 113, 'girl': 114}\n"
     ]
    }
   ],
   "source": [
    "word_to_idx={}\n",
    "i=0\n",
    "\n",
    "vocab = Counter(tokens)\n",
    "vocab = vocab.most_common()\n",
    "\n",
    "for (word, frequency) in vocab:\n",
    "    i = i+1\n",
    "    word_to_idx[word] = i\n",
    "\n",
    "print(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9817f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_encoder(tokens, word_to_idx):\n",
    "    encoded_idx = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        idx = word_to_idx[token]\n",
    "        encoded_idx.append(idx)\n",
    "        \n",
    "    return encoded_idx    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26bec03c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>integer_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 2, 40, 15, 6, 1, 9, 40, 15, 6, 1, 6, 8, 2, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[41, 42, 18, 43, 44, 45, 43, 46, 47, 24, 48, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[55, 2, 2, 55, 56, 57, 3, 56, 27, 3, 57, 58, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[60, 1, 6, 16, 1, 19, 1, 61, 1, 1, 1, 7, 19, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[31, 32, 65, 65, 31, 32, 66, 66, 31, 32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[67, 1, 3, 20, 68, 20, 20, 67, 3, 69, 70, 70, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[71, 72, 73, 33, 74, 72, 75, 34, 76, 35, 73, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[111, 1, 39, 14, 112, 1, 39, 11, 113, 11, 113,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[114, 114]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     integer_encoded\n",
       "0  [8, 2, 40, 15, 6, 1, 9, 40, 15, 6, 1, 6, 8, 2, 9]\n",
       "1                                             [2, 2]\n",
       "2  [41, 42, 18, 43, 44, 45, 43, 46, 47, 24, 48, 4...\n",
       "3  [55, 2, 2, 55, 56, 57, 3, 56, 27, 3, 57, 58, 5...\n",
       "4  [60, 1, 6, 16, 1, 19, 1, 61, 1, 1, 1, 7, 19, 2...\n",
       "5           [31, 32, 65, 65, 31, 32, 66, 66, 31, 32]\n",
       "6  [67, 1, 3, 20, 68, 20, 20, 67, 3, 69, 70, 70, ...\n",
       "7  [71, 72, 73, 33, 74, 72, 75, 34, 76, 35, 73, 3...\n",
       "8  [111, 1, 39, 14, 112, 1, 39, 11, 113, 11, 113,...\n",
       "9                                         [114, 114]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['integer_encoded'] = df['cleaned_tokens'].apply(lambda x : idx_encoder(x, word_to_idx))\n",
    "df[['integer_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45e90e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰의 최대 개수: 175\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(item) for item in df['integer_encoded'])\n",
    "\n",
    "print('토큰의 최대 개수:', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9aff51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens in df['integer_encoded']:\n",
    "    while len(tokens) < max_len:\n",
    "        tokens.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7566263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>integer_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 2, 40, 15, 6, 1, 9, 40, 15, 6, 1, 6, 8, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[41, 42, 18, 43, 44, 45, 43, 46, 47, 24, 48, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[55, 2, 2, 55, 56, 57, 3, 56, 27, 3, 57, 58, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[60, 1, 6, 16, 1, 19, 1, 61, 1, 1, 1, 7, 19, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[31, 32, 65, 65, 31, 32, 66, 66, 31, 32, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[67, 1, 3, 20, 68, 20, 20, 67, 3, 69, 70, 70, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[71, 72, 73, 33, 74, 72, 75, 34, 76, 35, 73, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[111, 1, 39, 14, 112, 1, 39, 11, 113, 11, 113,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[114, 114, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     integer_encoded\n",
       "0  [8, 2, 40, 15, 6, 1, 9, 40, 15, 6, 1, 6, 8, 2,...\n",
       "1  [2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2  [41, 42, 18, 43, 44, 45, 43, 46, 47, 24, 48, 4...\n",
       "3  [55, 2, 2, 55, 56, 57, 3, 56, 27, 3, 57, 58, 5...\n",
       "4  [60, 1, 6, 16, 1, 19, 1, 61, 1, 1, 1, 7, 19, 2...\n",
       "5  [31, 32, 65, 65, 31, 32, 66, 66, 31, 32, 0, 0,...\n",
       "6  [67, 1, 3, 20, 68, 20, 20, 67, 3, 69, 70, 70, ...\n",
       "7  [71, 72, 73, 33, 74, 72, 75, 34, 76, 35, 73, 3...\n",
       "8  [111, 1, 39, 14, 112, 1, 39, 11, 113, 11, 113,...\n",
       "9  [114, 114, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['integer_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0ab2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
